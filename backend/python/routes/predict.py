from fastapi import APIRouter, UploadFile, File, HTTPException, Request
import torch
import torchvision.transforms as transforms
from PIL import Image
import io
import os
import threading
import concurrent.futures
from collections import Counter
from services.servoControl import set_servo_command
from torchvision import models
import torch.nn as nn
import time
from datetime import datetime

router = APIRouter()

# processing_result = None  
# result_ready = False  
# processing_status = False  # C·ªù ƒë·ªÉ b√°o server ƒëang x·ª≠ l√Ω
# status_ready = False  # C·ªù b√°o ESP32-CAM d·ª´ng g·ª≠i ·∫£nh

# Th∆∞ m·ª•c l∆∞u ·∫£nh nh·∫≠n ƒë∆∞·ª£c
UPLOAD_FOLDER = os.path.join(os.path.dirname(__file__), "..", "received_images")
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

# Danh s√°ch nh√£n r√°c v√† ID servo (Th·ª© t·ª± kh·ªõp v·ªõi hu·∫•n luy·ªán)
TRASH_LABELS = ['metal', 'paper', 'plastic', 'trash_cardboard', 'trash_trash']
TRASH_CATEGORIES = {
    'metal': 0,
    'paper': 1,
    'plastic': 2,
    'trash_cardboard': 3,
    'trash_trash': 3  # G·ªôp chung 1 servo
}
num_classes = len(TRASH_LABELS)  # ƒê·∫£m b·∫£o ƒë√∫ng s·ªë l·ªõp

# Kh·ªüi t·∫°o m√¥ h√¨nh MobileNetV2 v·ªõi pretrained weights
def load_model():
    try:
        model = models.mobilenet_v2(pretrained=True)  # D√πng pretrained weights
        model.classifier = nn.Sequential(
            nn.Dropout(0.4),
            nn.Linear(model.last_channel, num_classes)
        )
        # ƒê∆∞·ªùng d·∫´n ƒë·∫øn m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán
        BASE_DIR = os.path.dirname(os.path.abspath(__file__))
        MODEL_PATH = os.path.join(BASE_DIR, "..", "models", "model.pth")

        # Ki·ªÉm tra n·∫øu t·ªáp m√¥ h√¨nh t·ªìn t·∫°i
        if not os.path.exists(MODEL_PATH):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y t·ªáp m√¥ h√¨nh t·∫°i: {MODEL_PATH}")
            
        model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu')))
        model.eval()  # Chuy·ªÉn m√¥ h√¨nh sang ch·∫ø ƒë·ªô ƒë√°nh gi√°
        return model
    except Exception as e:
        print(f"L·ªói khi t·∫£i m√¥ h√¨nh: {e}")
        raise

# T·∫£i m√¥ h√¨nh khi kh·ªüi ƒë·ªông server
try:
    model = load_model()
except Exception as e:
    print(f"Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh: {e}")
    model = None

# Ti·ªÅn x·ª≠ l√Ω ·∫£nh gi·ªëng `test.ipynb`
IMG_SIZE = 224
transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomRotation(30),  # Xoay ·∫£nh ng·∫´u nhi√™n
    transforms.RandomHorizontalFlip(),  # L·∫≠t ngang ·∫£nh
    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),  # C·∫Øt ng·∫´u nhi√™n
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Bi·∫øn l∆∞u k·∫øt qu·∫£
results = []
processing_status = False
status_ready = False
lock = threading.Lock()

def process_image(image_bytes):
    global results, processing_status, status_ready
    try:
        image = Image.open(io.BytesIO(image_bytes)).convert("RGB")
        img_tensor = transform(image).unsqueeze(0)

        with torch.no_grad():
            outputs = model(img_tensor)
            probs = torch.nn.functional.softmax(outputs, dim=1)
            confidence, predicted = torch.max(probs, 1)
            trash_type = TRASH_LABELS[predicted.item()]
            
            with lock:
                results.append(trash_type)
                if len(results) == 5:
                    final_result = Counter(results).most_common(1)[0][0]
                    set_servo_command(final_result, TRASH_CATEGORIES[final_result])
                    results.clear()
                    status_ready = True
                    processing_status = False
        
        return trash_type, confidence.item()
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Kh√¥ng th·ªÉ x·ª≠ l√Ω ·∫£nh: {str(e)}")
    
@router.post('/log')
async def receive_log(request: Request):
    log_data = await request.json()
    log_message = log_data.get("log")
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] üì° Log t·ª´ ESP32-CAM: {log_message}")
    return {"message": "Log nh·∫≠n th√†nh c√¥ng!"}


@router.post("/predict")
async def predict_trash(file: UploadFile = File(...)):
    try:
        # Ki·ªÉm tra n·∫øu m√¥ h√¨nh ƒë√£ t·∫£i
        if model is None:
            raise HTTPException(status_code=500, detail="M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng")
            
        # Ki·ªÉm tra lo·∫°i t·ªáp tin
        if not file.filename.lower().endswith(('.png', '.jpg', '.jpeg')):
            raise HTTPException(status_code=400, detail="Ch·ªâ ch·∫•p nh·∫≠n t·ªáp ·∫£nh .png, .jpg ho·∫∑c .jpeg")
        
        image_bytes = await file.read()
        
        # L∆∞u ·∫£nh v√†o th∆∞ m·ª•c
        timestamp = int(time.time())
        image_filename = f"{timestamp}.jpg"
        image_path = os.path.join(UPLOAD_FOLDER, image_filename)
        
        with open(image_path, "wb") as f:
            f.write(image_bytes)
        global processing_status
        processing_status = True
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            future = executor.submit(process_image, image_bytes)
            trash_type, confidence_value = future.result()
    
        return {
            "trash_type": trash_type,
            "confidence": f"{confidence_value:.2f}",
            "servo_id": TRASH_CATEGORIES[trash_type],
            "image_saved": image_filename
        }
        print(trash_type)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói khi x·ª≠ l√Ω: {str(e)}")  

@router.get('/check_status')
async def check_status():
    global status_ready
    if processing_status:
        return {"status": "processing"}, 202  
    elif status_ready:
        status_ready = False  
        return {"status": "done"}, 200  
    return {"status": "idle"}, 204   